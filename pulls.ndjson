{"pull_number":"19460","title":"model: support GLM MoE DSA arch (NOTE: indexer is not yet supported)","body":"Ref upstream vllm PR: https://github.com/vllm-project/vllm/pull/34124\r\n\r\n> [!IMPORTANT]  \r\n> This PR allows converting safetensors to GGUF while keeping the indexer tensors (for deepseek sparse attention), but they are left unused by the cpp code. **The quality will be suboptimal**  \r\n> Support for indexer tensor will be in a follow-up PR. The GGUF will NOT need to be generated again\r\n\r\nThe arch should be exactly the same as GlmMoeLite (aka GLM 4.7 Flash, PR: https://github.com/ggml-org/llama.cpp/pull/18936), but I'm taking time to properly moving it to a new arch while preserving the MTP tensors\r\n\r\n## Testing\r\n\r\nBecause the model is not public, I tried using GLM 4.7 Flash as the test subject.\r\n1. Download https://huggingface.co/zai-org/GLM-4.7-Flash\r\n2. Change the config.json: Glm4MoeLiteForCausalLM --> GlmMoeDsaForCausalLM\r\n3. Convert it to GGUF\r\n4. Test against the \"normal\" version of GLM 4.7 Flash GGUF (the one with deepseek2 arch)\r\n\r\nFrom my tests, `compare-logprobs.py` reports 0.0 differences between the two\r\n","pull_head_sha":"d8a465650c0be31ac51374543e9c8697baf30eba","loci_pr_branch":"loci/pr-19460-xsn-glm_dsa","short_merge_base":"292f690","loci_main_branch":"loci/main-292f690","use_loci_base":0}
