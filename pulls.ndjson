{"pull_number":"19378","title":"ggml: backend-agnostic tensor parallelism","body":"This PR adds support for backend-agnostic tensor parallelism, enabled via specifying `--split-mode tensor`. This is done by adding a new \"meta\" backend that internally wraps multiple \"simple\" backends but can be used in the same way as a regular ggml backend.\r\n\r\n### ggml Backend Interface Changes\r\n\r\nThis PR extends the ggml backend interface with some new functions for tensor copying:\r\n\r\n* `set_tensor_2d_async`/`get_tensor_2d_async` which are equivalent to `memcpy2DAsync` in CUDA. This is not needed for the computation of the meta backend itself but rather for setting/getting weights or the output. Currently not implemented, as a workaround the one-dimensional version is used in a loop.\r\n* `shfl_tensor_async` to allow two ggml backends to exchange two tensors and to synchronize on the completion of the exchange. As a fallback `cpy_tensor_async` can be used but this has a higher latency because the copy in one direction can only start once the one in the other direction has finished. Needed for a generic AllReduce between ggml backends. Implemented.\r\n* `allreduce_tensor_async` to allow ggml backends to specify a backend-specific way to do an AllReduce operation. Intended to be used for NCCL support in cooperation with @gaugarg-nv . Not yet implemented.\r\n\r\n@slaren please provide feedback regarding whether you agree that these operations should be in the ggml backend interface. For context, all of them are optional and can use existing operations as a fallback.\r\n\r\n### Meta Backend Implementation Details\r\n\r\nThe meta backend implements an entire ggml backend stack starting from a meta device. The meta device is created from multiple simple backend devices as well as a function to determine how the data should be split across devices (\"split states\"). Backend buffer types, buffers, and backends are created as per usual. When calling `ggml_backend_graph_compute` the code infers the split states of the nodes in the compute graph based on the split states assigned for the weights/kv cache. The basic pattern is to make all tensors mirrored by default. For the weight matrices, do a split in dimension 1, then a split in dimension 0, then an AllReduce. For a transformer this means two AllReduce operations, one after the attention and one after the FFN. The attention is effectively split by dimension 0, which equates to a split by attention head.\r\n\r\nAn generic AllReduce operation is performed in the meta backend by splitting the graph into subgraphs. After a subgraph is executed, call `shfl_tensor_async` to make backends exchange partial results, and then have them execute auxiliary graphs that contain only a `GGML_ADD` operation to combine the results.\r\n\r\nThe memory allocation for the compute graph is rather tricky - the way I solved it is to allocate the memory for the meta backend as per usual and to then transplant the calculated addresses relative to the backend buffer base pointer to the underlying simple backends. Because the simple tensors only require a fraction of the full memory this yields correct results, though it does result in overallocation for the compute graphs. For the weights/kv cache the memory allocation for the meta backend is done via a new function `ggml_backend_meta_alloc_ctx_tensors_from_buft` to prevent duplicated weights (which are much larger in size). I'm not yet sure what the best approach will be long-term, I think the graph allocation code in `ggml-alloc.c` will need to be adjusted.\r\n\r\n### Current Issues/Limitations\r\n\r\n* Only 1 or 2 GPUs are supported.\r\n* All GPUs must have an equal share of the data, `--tensor-split` has no effect.\r\n* Only dense models are supported. The LLaMA 3 models seem to be working correctly, I have not yet tested others.\r\n* Support for `llama_params_fit` is not implemented so the context size has to be set manually.\r\n* Without FlashAttention the code will probably crash because some transition between split states is not yet implemented.\r\n* In principle all backends should work. CUDA does in my testing, Vulkan however does not. I think there may be some issues with deadlock between the GPUs. @jeffbolznv @0cc4m if you could take a look it would be appreciated.\r\n* Memory for the ggml contexts is being overallocated.\r\n* Performance is (presumably) still suboptimal vs. NCCL.\r\n* I'm currently using tensor names to determine how to split individual tensors. I think it would be preferable to use some sort of enum instead (which we already seem to have for loading tensors). This should also be used for `llama_params_fit`.\r\n* I'm currently setting `ggml_tensor::data` to dummy values since that is what is checked in `ggml-alloc.c` to determine whether or not a tensor is considered allocated. This dummy value should never actually be used for any computations but I don't consider this a good solution.\r\n* I'm not putting meta devices into the ggml backend device registry (which I think is correct).\r\n\r\n### Performance\r\n\r\n<details>\r\n<summary>LLaMA 3 on 2x RTX 4090</summary>\r\n\r\n| model                          | test             | t/s -sm layer | t/s -sm row | t/s -sm tensor |\r\n| ------------------------------ | --------------:  | ------------: | ----------: | -------------: |\r\n| llama 8B Q4_0                  | pp512            | **12550.97**  |     2997.41 |        6305.68 |\r\n| llama 8B Q4_0                  | pp2048           | **18788.11**  |     2970.83 |        6300.12 |\r\n| llama 8B Q4_0                  | tg128            | **175.43**    |       67.00 |         101.98 |\r\n| llama 8B Q4_0                  | pp512 @ d32768   | **5099.65**   |     2200.50 |        4137.73 |\r\n| llama 8B Q4_0                  | pp2048 @ d32768  | **7925.21**   |     2242.84 |        4337.54 |\r\n| llama 8B Q4_0                  | tg128 @ d32768   | 96.78         |       49.76 |     **102.81** |\r\n| llama 8B Q4_0                  | pp512 @ d65536   | **3154.69**   |     1748.05 |        3139.40 |\r\n| llama 8B Q4_0                  | pp2048 @ d65536  | **4996.19**   |     1806.82 |        3404.70 |\r\n| llama 8B Q4_0                  | tg128 @ d65536   | 67.11         |       40.27 |      **83.62** |\r\n| llama 8B Q4_0                  | pp512 @ d131072  | 1800.72       |     1243.57 |        2152.01 |\r\n| llama 8B Q4_0                  | pp2048 @ d131072 | **2867.83**   |     1294.61 |        2238.01 |\r\n| llama 8B Q4_0                  | tg128 @ d131072  | 41.66         |       29.19 |      **59.53** |\r\n| llama 8B F16                   | pp512            | **10578.54**  |     1591.55 |        5950.44 |\r\n| llama 8B F16                   | pp2048           | **15890.45**  |     1581.04 |        6005.96 |\r\n| llama 8B F16                   | tg128            | 60.07         |       46.32 |      **70.64** |\r\n| llama 8B F16                   | pp512 @ d32768   | **4745.19**   |     1330.02 |        4032.19 |\r\n| llama 8B F16                   | pp2048 @ d32768  | **7329.28 **  |     1347.02 |        4279.02 |\r\n| llama 8B F16                   | tg128 @ d32768   | 47.03         |       38.02 |      **64.63** |\r\n| llama 8B F16                   | pp512 @ d65536   | 3033.10       |     1154.54 |    **3100.10** |\r\n| llama 8B F16                   | pp2048 @ d65536  | **4782.02**   |     1176.88 |        3341.03 |\r\n| llama 8B F16                   | tg128 @ d65536   | 38.72         |       32.19 |      **56.48** |\r\n| llama 8B F16                   | pp512 @ d131072  | 1735.63       |      905.39 |    **2090.03** |\r\n| llama 8B F16                   | pp2048 @ d131072 | **2782.42**   |      936.55 |        2288.65 |\r\n| llama 8B F16                   | tg128 @ d131072  | 28.60         |       24.79 |      **43.32** |\r\n| llama 70B Q3_K - Small         | pp512            | 1287.50       |      582.81 |    **1072.80** |\r\n| llama 70B Q3_K - Small         | pp2048           | **1954.83**   |      590.45 |        1069.28 |\r\n| llama 70B Q3_K - Small         | tg128            | 27.97         |       20.36 |      **29.65** |\r\n| llama 70B Q3_K - Small         | pp512 @ d32768   | 776.80        |      458.17 |     **812.77** |\r\n| llama 70B Q3_K - Small         | pp2048 @ d32768  | **1185.82**   |      459.43 |         824.99 |\r\n| llama 70B Q3_K - Small         | tg128 @ d32768   | 20.85         |       16.19 |      **29.39** |\r\n\r\n</details>\r\n\r\nGenerally speaking it can be observed that parallelizing larger models has better performance than parallelizing smaller models. Similarly, parallelizing the model becomes more worthwhile as the context depth increases. This makes sense as both of these result in a larger workload per GPU vs. the overhead from parallelization. Token generation benefits more from parallelization than prompt processing because the amount of data that needs to be transferred between GPUs is proportional to batch size - long-term it may make sense to implement support for FP16/BF16 compute types which would count the I/O in half vs. FP32. For pp512 pipeline parallelism is effectively disabled while for pp2048 it's enabled. With pipeline parallelism `-sm layer` is still faster than `-sm tensor` even at high context depths.","pull_head_sha":"02ee504f90fe6bcf756661d0db16ce9a9667a960","loci_pr_branch":"loci/pr-19378-ggml-meta-backend-8","short_merge_base":"22cae83","loci_main_branch":"loci/main-22cae83","use_loci_base":0}
