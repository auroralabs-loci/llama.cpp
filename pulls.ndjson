{"pull_number":"17791","title":"ggml-cpu: add repack GEMM and GEMV for floating-point","body":"## Summary\r\nThis PR adds repacking and GEMM/GEMV kernels for floating-point (FP16  and FP32) for RVV (with the `zvfh` extension).\r\n\r\n## Key Changes\r\n- Added RVV kernels for GEMM with tiling:\r\n  - `7 x {16, 32, 64, 128}` (selected based on VLEN)\r\n- Added RVV kernels for GEMV with tiling:\r\n  - `1 x {16, 32, 64, 128}` (selected based on VLEN)\r\n- Added scalar functions for repacking. They support arbitrary tile sizes.  \r\n- Generic scalar fallbacks for GEMM/GEMV operations. \r\n- `ggml_quantize_mat_t` is refactored to `ggml_repack_mat_t` to allow for a common interface for both quantization and floating-point repacking.\r\n- Additional template parameter `NB_ROWS` added to select the number of rows to interleave for repacking. Previously, this was fixed at `4`.\r\n\r\n## Tile Sizes\r\n\r\nThe repack operation interleaves `N` rows of `activations` with an interleave size of `K`, and `M` columns of `weights` with an interleave size of `K`.\r\n\r\n`NxK` is fixed at `7x1`. This introduces 7 accumulators with `LMUL=4` (7 x 4 = 28 registers), each accumulating `M` results.\r\n\r\n`M` is varied based on the available VLEN:\r\n\r\n| VLEN | Tile Size (N x M x K) |\r\n| -------- | ----------- |\r\n| 128 | 7 x 16 x 1 |\r\n| 256 | 7 x 32 x 1 |\r\n| 512 | 7 x 64 x 1 |\r\n| 1024 | 7 x 128 x 1 |\r\n\r\n`M` is the maximum number of values that can be loaded in (LMUL=2 for F16, LMUL=4 for F32).\r\n  \r\n## Testing\r\nKernels were functionally tested on QEMU for VLENs (128-bit, 256-bit, 512-bit and 1024-bit) for a range of input sizes.\r\n\r\n## Benchmarking Results\r\nEnd-to-end benchmarking on  `BananaPI-BPI F3 (VLEN=256)` with `llama-bench` (`Threads=8)`).\r\n\r\n### Prefill / Prompt Processing (GEMM)\r\n\r\n#### Tokens / Second\r\n\r\n| Model | Prompt Size | Repack GEMM (7x32) | Vec Dot |\r\n| --------- | ----------------- | ------------------------------- | ----------- |\r\nTinyllama F16 1.1B | 28 | 24.72 | 8.31\r\nTinyllama F16 1.1B | 32 | 16.72 | 8.42\r\nTinyllama F16 1.1B | 64 | 22.55 | 8.57\r\nTinyllama F16 1.1B | 128 | 22.78 | 8.78\r\nTinyllama F16 1.1B | 256 | 21.82 | 8.57\r\nTinyllama F16 1.1B | 512 | 21.81 | 8.68\r\n\r\n| Model | Prompt Size | Repack GEMM (7x32) | Vec Dot |\r\n| --------- | ----------------- | ------------------------------- | ----------- |\r\nTinyllama F32 1.1B | 28 | 11.45 | 3.72\r\nTinyllama F32 1.1B | 32 | 7.13 | 3.75\r\nTinyllama F32 1.1B | 64 | 10.76 | 3.74\r\nTinyllama F32 1.1B | 128 | 10.86 | 3.73\r\nTinyllama F32 1.1B | 256 | 10.94 | 3.68\r\nTinyllama F32 1.1B | 512 | 11.12 | 3.79\r\n\r\n**Result**:  ~2x-3x speedup over `vec_dot`\r\n\r\n### Decode (GEMV)\r\n\r\n#### Tokens / Second\r\n\r\n| Model | Decode Size (Prompt=32) | Repack GEMV (1x32) | Vec Dot |\r\n| --------- | ------------------------------------- | ------------------------------- | ----------- |\r\nTinyllama F16 1.1B | 10 | 3.37 | 3.11\r\nTinyllama F16 1.1B | 16 | 3.29 | 3.45\r\nTinyllama F16 1.1B | 32 | 3.12 | 3.25\r\nTinyllama F16 1.1B | 64 | 3.23 | 3.27\r\nTinyllama F16 1.1B | 100 | 3.04 | 3.15\r\nTinyllama F16 1.1B | 128 | 3.09 | 3.2\r\nTinyllama F16 1.1B | 256 | 3.15 | 3.19\r\n\r\n| Model | Decode Size (Prompt=32) | Repack GEMV (1x32) | Vec Dot |\r\n| --------- | ------------------------------------- | ------------------------------- | ----------- |\r\nTinyllama F32 1.1B | 10 | 1.66 | 1.74\r\nTinyllama F32 1.1B | 16 | 1.73 | 1.63\r\nTinyllama F32 1.1B | 32 | 1.81 | 1.68\r\nTinyllama F32 1.1B | 64 | 1.61 | 1.69\r\nTinyllama F32 1.1B | 100 | 1.72 | 1.75\r\nTinyllama F32 1.1B | 128 | 1.76 | 1.72\r\nTinyllama F32 1.1B | 256 | 1.75 | 1.69\r\n\r\n**Result**: No noticeable improvement, as decode remains memory-bound.\r\n\r\n## Additional Notes\r\n- Current fallback model requires every architecture to have a scalar fallback for each implementation. This creates a clutter in `arch-fallback.h` as `7xMx1` is very RVV-specific tiling, and should not be used by other architectures.\r\n- `GEMM` reaches peak performance when the prompt is a multiple of 7 (for example, `prompt=28`). To handle leftovers, it defaults to `GEMV`, which impacts performance. Ideally, there should be leftover `Nx32` kernels which handle each leftover case from `2-6` leftover tokens. \r\n\r\n## Future Work\r\nSubsequent PRs plan to add RVV kernels for quantization types, as well as extend existing quantization support to other VLENs.\r\n\r\n## References\r\nThe selection of the tiling `7x32x1` is based off of the `mmt4d` kernel in IREE for RVV: https://github.com/iree-org/iree/pull/20263","pull_head_sha":"28e07aad92e646adc4a9f3f77f99d129185d07f8","loci_pr_branch":"loci/pr-17791-10x-repack-fp","short_merge_base":"c0204a0","loci_main_branch":"loci/main-c0204a0","use_loci_base":0}
